\section{GMM}
We introduce the GMM algorithm which needs to be implemented in this task. The EM will be used on a two dimensional feature set, which consists of 4800 samples for training and 300 for testing.

The GMM algorithm which was implemented uses the standard initialization techniques.
GMMs are basically a sum of super positions of $C$ Gaussians. The distribution of each Gaussian component $c$ is unknown or \textbf{hidden}. The basic formulation for a component $c$ drawn from a GMM with mean $\boldsymbol{\mu}$, covariance matrix $\mathbf{\Sigma}$, observation data $\mathbf{x}$ and dimension $d$ is given by:
\begin{equation}
P(\mathbf{x}|\boldsymbol{\mu},\mathbf{\Sigma}) =\frac{\displaystyle e^{-\frac{1}{2} \displaystyle ( \mathbf{x}-\boldsymbol{\mu})^{'} \mathbf{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu}) } } {\displaystyle (2\pi)^{\frac{d}{2}} |\mathbf{\Sigma}|^{\frac{1}{2}}} 
\end{equation}
Furthermore a \textbf{hidden} variable $\mathbf{h}$ is introduced, which indicates which Gaussian component did generate the data.
\begin{equation*}
\begin{split}
\mathbf{h} &= (0,0, \ldots ,1, \ldots ,0) \\
|\mathbf{h}| &= |C|
\end{split}
\end{equation*}
Since a GMM represents a probability distribution, it is necessary to give every Gaussian component a weight $w_c$ or differently said a probability that this component did generate the output $P( h_c = 1 ) = w_c$. This weight helps to distinguish which mixture generated an observed output, since it can be seen as an importance factor for every mixture. Low weight components are unlikely to generate any data.
The probability distribution of a GMM is given by:
\begin{equation}
\label{eq:norm_gmm}
\begin{split}
P( \mathbf{x} ) &= \sum\limits_c^C P ( h_c = 1 ) P( \mathbf{x} | h_c = 1 )  \\
&= \sum\limits_c^C w_c \mathcal{N}( \mathbf{x} |\boldsymbol{\mu}_c,\mathbf{\Sigma}_c)\\
&\sum\limits_c^C w_c = 1 
\end{split}
\end{equation}
More generally, a GMM has $C \times |\theta|$ parameters. $\theta$ refers to the set of parameters given the current component $c$ : $\theta(c) = \left \{ {w_c,\boldsymbol{\mu}_c,\mathbf{\Sigma}_c}\right \}$.


\section{EM Algorithm}
EM is the standard algorithm to solve the GMM's parameter estimation problem. It iteratively calculates better estimates of the given parameters $\theta$ values by using MLE(or MAP) criterion and maximizes the calculated parameters. 
%f(x|t) = log f (x _1 .. | t)
Suppose some data $\chi$ is observed and the function which generated it is known (or assumed). Furthermore a set of parameters $\theta$ is known (constant), which generates the data $\chi$ in some kind of unknown mixture.
If one wants to estimate the best set of parameters $\theta$ which most likely explain the evidence (the observation), EM should be used. 
Let $\mathbf{x} \in \chi$ denote a observation vector. The likelihood that the data is generated by the set of parameters $\theta$ is:
\begin{equation}
\label{eq:singvecmle}
\mathcal{L}(\theta) = P \left( \mathbf{x} | \theta \right)
\end{equation}

Since generally the observation consists of not only a single vector, but rather out of $N$ observed data points, one can rewrite the expression into:

\begin{equation}
\label{eq:xntotheta}
\mathcal{L} \left( \theta \right) = P \left( \mathbf{x}_1 , \ldots , \mathbf{x}_N | \theta \right)
\end{equation}
If an assumption is made that the observed data vectors are independent from each other, the equation can be rewritten to:
\begin{equation*}
\mathcal{L} \left( \theta \right) = \prod\limits_n^N P \left( \mathbf{x}_n | \theta \right)
\end{equation*}
Finally the log is used to replace the product by a sum. The result is the so called log likelihood.

\begin{equation}
\label{eq:loglikelihood}
\mathcal{L} \left( \theta \right) = \sum\limits_n^N  \log \left( P \left( \mathbf{x}_n | \theta \right) \right)
\end{equation}

M is a method to maximize both, the visible and hidden parts of data.  
This is done by iteratively using the current value of the parameters to estimate a distribution over the hidden variables given the observed part (E-step) and then finding the parameters that maximize the likelihood of the visible observation $\chi$ and the estimated hidden parameters from the E-step (M-step).

\begin{equation}
\label{eq:gmm_normlike}
\mathcal{L} \left(\theta \right) = \sum\limits_n^N \log \left( \sum\limits_c^C w_c \mathcal{N}\left(\mathbf{x}_n |\boldsymbol{\mu}_c,\boldsymbol{\Sigma}_c \right) \right)
\end{equation}

Finally the EM algorithm:
\begin{algorithm}
\caption{EM Algorithm}
\KwResult{Estimated parameters for $\theta$: $\hat\theta$}
Initialize $\theta$; \\
previous = Calculate Log Likelihood $Q(\theta)$; \\
\While{ new - previous > threshold }{
	Do E-step; \\
	$\hat\theta$=Do M-Step; \\
	new = Calculate Log Likelihood $Q(\hat\theta)$; \\
}
\end{algorithm}

\paragraph*{E-step}
The E-step calculates an auxiliary function, which is the lower bound of the formula seen above. This function is an indicator for the EM to either proceed the calculation or stop. This auxiliary function is usually referred to as the log likelihood.

\begin{equation}
\label{eq:llk}
\begin{split}
\mathcal{Q} \left(\theta;\hat{\theta} \right) =& \sum\limits_n^N \sum\limits_c^C \gamma_{c}(n) \log \left(w_c \right) \\ 
& - \frac{1}{2} \sum\limits_n^N \sum\limits_c^C \gamma_{c}(n) \left( \log |\mathbf{\Sigma}_c| + \left(\mathbf{x}_n - {\boldsymbol{\mu}_c} \right)^{'} \mathbf{\Sigma}_c^{-1} \left( \mathbf{x}_n - {\boldsymbol{\mu}_c}\right)\right)
\end{split}
\end{equation}

The posterior probability of a data vector $\mathbf{x}_n$ belonging to a component $c$ needs to be evaluated.

\begin{equation}
\gamma_c^k(n) = \frac{w_c^{k-1} \mathcal{N} \left( \mathbf{x}_n | \boldsymbol{\mu}_c^{k-1} , \mathbf{\Sigma}_c^{k-1} \right)}{
\sum\limits_i^C w_i^{k-1} \mathcal{N} \left( \mathbf{x}_n | \boldsymbol{\mu}_i^{k-1}, \mathbf{\Sigma}_i^{k-1}\right)}
\end{equation}

$\gamma_c^k(n)$ is the probability that component $c$ belongs to dataset $n$ at iteration $k$. If this calculation is done, the new probabilities for every component need to be evaluated, using the log likelihood.
\paragraph*{M-step}
The M-step updates the previous parameters by maximizing their posterior probability, using the estimates from the E-step.
\begin{equation}
\begin{split}
\gamma_c &= \sum\limits_n^N \gamma_{c}(n) \\
\hat{\boldsymbol{\mu}_c} &=\frac{1}{\gamma_c(n)} \sum\limits_n^N \gamma_c \mathbf{x}_n \\
\hat{\mathbf{\Sigma}_c} &=\frac{1}{\gamma_c(n)} \sum\limits_n^N \gamma_c \left( \mathbf{x}_n - \hat{\boldsymbol{\mu}_c} \right)^{'} \left( \mathbf{x}_n - \hat{\boldsymbol{\mu}_c} \right) \\
w_c &= \frac{\gamma_c}{\sum\limits_c^C \gamma_c} \\
\end{split}
\end{equation}

After the parameters are updated in the M-Step, the new parameters (denoted by $\hat{a} $) will be re-estimated using the E-step.

\section{Improving the performance}
In our experiments it can be seen that the initialization of mean and variance plays a huge role how if the EM will find a proper minima or not. We used random initialization in the interval of $ [0,1]$ for mean and variance, which lead both to huge differences. 
In the end we fixed out initialization on using the mean and variance of the data and adding some Gaussian noise to it. This procedure is stable, as in our experiments the convergence is guaranteed.
Although there is a problem when using too many classes within a GMM. One of the most troublesome is the singularity of the covariance matrix. It can happen that due to too many classes the values within the covariance matrix are too close to zero, which would lead to a zero determinant. To avoid this problem one could use flooring, so that values lower than a flooring constant $f$, will be replaced by that flooring constant.

\section{Used GMM}
The implemented GMM works as follows:
\begin{enumerate}
\item Train a GMM with $n$ components for each classlabel
\item Test GMM against the given test data
\end{enumerate}
We initialized the GMM by using the data means. 
At the end, we used 2, 5,6,10 classes respectively. The results can be seen in 10class.txt, 2class.txt, 5class.txt and 6class.txt.

\section{Other approaches}
One could easily use a more sophisticated method to initialize the EM. For example k-means or k-NN are the favourite ones, yet we did not attack these algorithms, since their runtime influences the runtime of our program heavily. Moreover one could use a MAP based approach during the update phase of the training to achieve a greater stability.
Since the amount of data is quite small, both of these approaches are too expensive in their runtime / implementation time, to really pay off for their cost.